#!/bin/bash
# Enhanced Analysis: 130 samples with baseline comparison and retry tracking

echo "=== ENHANCED TIER ANALYSIS (130 SAMPLES) ==="
echo ""

# Count total generations
TOTAL_GENS=$(tail -6000 /home/kobe/.pm2/logs/trellis-microservice-out.log | grep -c "RAW TRELLIS OPACITY")
echo "Total generations logged: $TOTAL_GENS"

if [ $TOTAL_GENS -lt 100 ]; then
    echo "âš ï¸  Not enough samples (found $TOTAL_GENS, expected 130)"
    exit 1
fi

# Python enhanced analysis
cat > /tmp/analyze_enhanced_tiers.py << 'PYEOF'
import re
import json
import statistics
from pathlib import Path
from collections import defaultdict

def parse_opacity_data():
    """Extract opacity statistics"""
    opacities = []
    with open('/tmp/tier_opacity_data.txt', 'r') as f:
        lines = f.readlines()
        for line in lines:
            if 'mean:' in line:
                match = re.search(r'mean: ([\d\.\-]+)', line)
                if match:
                    opacities.append(float(match.group(1)))
    return opacities

def parse_saved_ply_corruption():
    """Check for inf/nan in saved PLY"""
    corrupted = []
    with open('/tmp/tier_saved_ply.txt', 'r') as f:
        lines = f.readlines()
        for line in lines:
            if 'mean:' in line:
                if 'inf' in line or 'nan' in line:
                    corrupted.append(True)
                else:
                    corrupted.append(False)
    return corrupted

def parse_gaussians():
    """Extract gaussian counts"""
    gaussians = []
    with open('/tmp/tier_gaussians.txt', 'r') as f:
        for line in f:
            match = re.search(r'Gaussians: ([\d,]+)', line)
            if match:
                count = int(match.group(1).replace(',', ''))
                gaussians.append(count)
    return gaussians

def parse_gen_times():
    """Extract generation times"""
    times = []
    with open('/tmp/tier_gen_times.txt', 'r') as f:
        for line in f:
            match = re.search(r'completed in ([\d\.]+)s', line)
            if match:
                times.append(float(match.group(1)))
    return times

def parse_retry_attempts():
    """Parse retry attempts from gen-worker-1 logs"""
    retries = []
    try:
        with open('/tmp/tier_retries.txt', 'r') as f:
            for line in f:
                # Look for patterns like "SPARSE GENERATION" followed by "Retry"
                if 'SPARSE GENERATION' in line or 'Retrying' in line:
                    match = re.search(r'(\d+,?\d*) < (\d+,?\d*)', line)
                    if match:
                        first = int(match.group(1).replace(',', ''))
                        threshold = int(match.group(2).replace(',', ''))
                        retries.append({'first': first, 'threshold': threshold})
    except FileNotFoundError:
        pass
    return retries

def analyze_tier(data, tier_name, start_idx, end_idx):
    """Analyze metrics for a specific tier"""
    tier_opacities = data['opacities'][start_idx:end_idx]
    tier_gaussians = data['gaussians'][start_idx:end_idx]
    tier_times = data['times'][start_idx:end_idx]
    tier_corrupted = data['corrupted'][start_idx:end_idx] if start_idx < len(data['corrupted']) else []

    if not tier_opacities:
        return {}

    # Corruption analysis
    corruption_rate = sum(tier_corrupted) / len(tier_corrupted) if tier_corrupted else 0
    problematic = sum(1 for o in tier_opacities if o < 4.0)
    negative = sum(1 for o in tier_opacities if o < 0)

    result = {
        'samples': len(tier_opacities),
        'avg_gaussians': int(statistics.mean(tier_gaussians)) if tier_gaussians else 0,
        'median_gaussians': int(statistics.median(tier_gaussians)) if tier_gaussians else 0,
        'min_gaussians': min(tier_gaussians) if tier_gaussians else 0,
        'max_gaussians': max(tier_gaussians) if tier_gaussians else 0,
        'avg_opacity_mean': round(statistics.mean(tier_opacities), 4) if tier_opacities else 0,
        'opacity_std': round(statistics.stdev(tier_opacities), 4) if len(tier_opacities) > 1 else 0,
        'problematic_opacity_rate': round(problematic / len(tier_opacities), 3) if tier_opacities else 0,
        'negative_opacity_count': negative,
        'avg_gen_time': round(statistics.mean(tier_times), 2) if tier_times else 0,
        'median_gen_time': round(statistics.median(tier_times), 2) if tier_times else 0,
        'high_density_rate': round(sum(1 for g in tier_gaussians if g >= 400000) / len(tier_gaussians), 3) if tier_gaussians else 0,
        'low_density_rate': round(sum(1 for g in tier_gaussians if g < 150000) / len(tier_gaussians), 3) if tier_gaussians else 0,
        'corruption_rate_inf_nan': round(corruption_rate, 3),
    }

    return result

def main():
    print("Parsing log data...")

    # Extract data files
    import subprocess
    subprocess.run("tail -6000 /home/kobe/.pm2/logs/trellis-microservice-out.log | grep 'RAW TRELLIS OPACITY (before' -A 3 > /tmp/tier_opacity_data.txt", shell=True)
    subprocess.run("tail -6000 /home/kobe/.pm2/logs/trellis-microservice-out.log | grep 'SAVED PLY OPACITY' -A 2 > /tmp/tier_saved_ply.txt", shell=True)
    subprocess.run("tail -6000 /home/kobe/.pm2/logs/trellis-microservice-out.log | grep 'Gaussians:' > /tmp/tier_gaussians.txt", shell=True)
    subprocess.run("tail -6000 /home/kobe/.pm2/logs/trellis-microservice-out.log | grep 'Generation completed in' > /tmp/tier_gen_times.txt", shell=True)
    subprocess.run("tail -3000 /home/kobe/.pm2/logs/gen-worker-1-out.log | grep -E 'SPARSE|Retry' > /tmp/tier_retries.txt", shell=True)

    data = {
        'opacities': parse_opacity_data(),
        'corrupted': parse_saved_ply_corruption(),
        'gaussians': parse_gaussians(),
        'times': parse_gen_times(),
        'retries': parse_retry_attempts()
    }

    total_samples = min(len(data['opacities']), len(data['gaussians']), len(data['times']))

    print(f"Analyzing {total_samples} samples...")
    print(f"Retry attempts detected: {len(data['retries'])}")

    # Assume first 10 are baseline, then 4 tiers of 30 each
    results = {}

    if total_samples >= 40:  # At least baseline + 1 tier
        results['tier5_baseline_NO_FIXES'] = analyze_tier(data, 'Baseline', 0, 10)

        tier_size = 30
        results['tier1_simple'] = analyze_tier(data, 'Tier 1', 10, 10 + tier_size)
        results['tier2_moderate'] = analyze_tier(data, 'Tier 2', 10 + tier_size, 10 + tier_size * 2)
        results['tier3_complex'] = analyze_tier(data, 'Tier 3', 10 + tier_size * 2, 10 + tier_size * 3)
        results['tier4_very_complex'] = analyze_tier(data, 'Tier 4', 10 + tier_size * 3, min(total_samples, 10 + tier_size * 4))

    # Overall analysis (production tiers only, exclude baseline)
    prod_start = 10  # Skip baseline
    all_opacities = data['opacities'][prod_start:total_samples]
    all_gaussians = data['gaussians'][prod_start:total_samples]
    all_times = data['times'][prod_start:total_samples]
    all_corrupted = data['corrupted'][prod_start:total_samples] if len(data['corrupted']) > prod_start else []

    results['overall_production'] = {
        'total_samples': total_samples - 10,  # Exclude baseline
        'normalization_rate': 'See logs',  # Will be calculated from grep
        'corruption_eliminated': round(1 - (sum(all_corrupted) / len(all_corrupted)), 3) if all_corrupted else 1.0,
        'avg_gaussians': int(statistics.mean(all_gaussians)) if all_gaussians else 0,
        'avg_opacity_mean': round(statistics.mean(all_opacities), 4) if all_opacities else 0,
        'avg_gen_time': round(statistics.mean(all_times), 2) if all_times else 0,
        'high_density_overall': round(sum(1 for g in all_gaussians if g >= 400000) / len(all_gaussians), 3) if all_gaussians else 0,
        'retry_attempts': len(data['retries']),
        'ready_for_mainnet': (sum(all_corrupted) / len(all_corrupted)) < 0.05 if all_corrupted else True
    }

    # Retry effectiveness analysis
    if data['retries']:
        retry_improvements = [r.get('threshold', 150000) - r['first'] for r in data['retries'] if 'first' in r]
        if retry_improvements:
            results['retry_analysis'] = {
                'total_retries': len(data['retries']),
                'avg_improvement': f"+{int(statistics.mean(retry_improvements)):,} gaussians",
                'retry_success_rate': 'See logs for accept/reject',
            }

    # Save to JSON
    output_path = '/home/kobe/404-gen/tier_analysis_130_enhanced.json'
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2)

    print(f"âœ… Analysis complete! Results saved to: {output_path}")

    # Print summary
    print("\n" + "="*70)
    print("ENHANCED TIER ANALYSIS SUMMARY")
    print("="*70)

    # Baseline comparison
    if 'tier5_baseline_NO_FIXES' in results:
        baseline = results['tier5_baseline_NO_FIXES']
        print(f"\nðŸ”´ BASELINE (NO FIXES) - Proves fixes work:")
        print(f"  Corruption rate: {baseline.get('corruption_rate_inf_nan', 0)*100:.1f}%")
        print(f"  Avg opacity: {baseline.get('avg_opacity_mean', 0):.4f}")
        print(f"  Avg gaussians: {baseline.get('avg_gaussians', 0):,}")
        print(f"  >>> This is what happens WITHOUT our fixes <<<")

    # Production tiers
    for tier_key, tier_name in [
        ('tier1_simple', 'Tier 1: Simple (1-3 words)'),
        ('tier2_moderate', 'Tier 2: Moderate (4-8 words)'),
        ('tier3_complex', 'Tier 3: Complex (9-12 words)'),
        ('tier4_very_complex', 'Tier 4: Very Complex (13-16 words)')
    ]:
        if tier_key not in results:
            continue
        tier = results[tier_key]
        print(f"\nâœ… {tier_name}")
        print(f"  Avg Gaussians: {tier['avg_gaussians']:,} (median: {tier['median_gaussians']:,})")
        print(f"  Corruption: {tier.get('corruption_rate_inf_nan', 0)*100:.1f}%")
        print(f"  Avg Opacity: {tier['avg_opacity_mean']:.4f} Â± {tier['opacity_std']:.4f}")
        print(f"  Gen Time: {tier['avg_gen_time']:.2f}s")
        print(f"  High Density (>400K): {tier['high_density_rate']*100:.1f}%")

    # Overall
    print(f"\n{'='*70}")
    print("OVERALL PRODUCTION METRICS (WITH FIXES)")
    print("="*70)
    overall = results['overall_production']
    print(f"  Total Samples: {overall['total_samples']}")
    print(f"  Corruption Eliminated: {overall['corruption_eliminated']*100:.1f}%")
    print(f"  Avg Gaussians: {overall['avg_gaussians']:,}")
    print(f"  Avg Gen Time: {overall['avg_gen_time']:.2f}s")
    print(f"  High Density Rate: {overall['high_density_overall']*100:.1f}%")
    print(f"  Retry Attempts: {overall.get('retry_attempts', 0)}")
    print(f"  Ready for Mainnet: {overall['ready_for_mainnet']}")

    # Retry analysis
    if 'retry_analysis' in results:
        print(f"\n{'='*70}")
        print("RETRY EFFECTIVENESS")
        print("="*70)
        retry = results['retry_analysis']
        print(f"  Total Retries: {retry['total_retries']}")
        print(f"  Avg Improvement: {retry['avg_improvement']}")
        print(f"  >>> Retry logic helps sparse prompts generate more gaussians <<<")

    print("="*70)

if __name__ == '__main__':
    main()
PYEOF

# Run Python analysis
python3 /tmp/analyze_enhanced_tiers.py

# Show full JSON
echo ""
echo "ðŸ“„ Full JSON results:"
cat /home/kobe/404-gen/tier_analysis_130_enhanced.json | python3 -m json.tool

# Additional metrics from logs
echo ""
echo "ðŸ“Š Additional Metrics from Logs:"
echo ""

NORM_COUNT=$(tail -6000 /home/kobe/.pm2/logs/trellis-microservice-out.log | grep -c "ðŸ”§ Opacity corruption risk detected")
echo "  Opacity normalizations applied: $NORM_COUNT"

FILTERED=$(tail -3000 /home/kobe/.pm2/logs/gen-worker-1-out.log | grep -c "FILTERED")
echo "  Quality gate filters: $FILTERED"

TIMEOUT=$(tail -3000 /home/kobe/.pm2/logs/gen-worker-1-out.log | grep -c "TIMEOUT RISK")
echo "  Timeout filters: $TIMEOUT"

echo ""
echo "âœ… Enhanced analysis complete!"
